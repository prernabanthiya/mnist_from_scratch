# mnist_from_scratchâ€“ No TensorFlow, No PyTorch!
This repository contains a 2-layer neural network built from scratch using only NumPy, without any deep learning frameworks like TensorFlow or PyTorch. The goal is to understand the mathematics behind neural networks, including forward propagation, backpropagation, activation functions, and gradient descent.
## ğŸš€ Features  
âœ… Implements **forward propagation, backpropagation, and gradient descent** manually  
âœ… Uses **ReLU activation** for hidden layers and **softmax** for the output layer  
âœ… Achieves **84% accuracy** on the MNIST dataset  
âœ… **No pre-built deep learning libraries** â€“ Just NumPy & Maths!  

## ğŸ“Š Dataset  
The dataset can be downloaded from Kaggle:  
ğŸ“¥ [MNIST Digit Recognizer](https://www.kaggle.com/competitions/digit-recognizer/data)  

## ğŸ— Network Architecture  
- **Input Layer:** 784 neurons (28x28 flattened image)  
- **Hidden Layer:** 128 neurons (ReLU activation)  
- **Output Layer:** 10 neurons (Softmax activation) 

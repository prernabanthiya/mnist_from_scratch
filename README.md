# mnist_from_scratch– No TensorFlow, No PyTorch!
This repository contains a 2-layer neural network built from scratch using only NumPy, without any deep learning frameworks like TensorFlow or PyTorch. The goal is to understand the mathematics behind neural networks, including forward propagation, backpropagation, activation functions, and gradient descent.
## 🚀 Features  
✅ Implements **forward propagation, backpropagation, and gradient descent** manually  
✅ Uses **ReLU activation** for hidden layers and **softmax** for the output layer  
✅ Achieves **84% accuracy** on the MNIST dataset  
✅ **No pre-built deep learning libraries** – Just NumPy & Maths!  

## 📊 Dataset  
The dataset can be downloaded from Kaggle:  
📥 [MNIST Digit Recognizer](https://www.kaggle.com/competitions/digit-recognizer/data)  

## 🏗 Network Architecture  
- **Input Layer:** 784 neurons (28x28 flattened image)  
- **Hidden Layer:** 128 neurons (ReLU activation)  
- **Output Layer:** 10 neurons (Softmax activation) 
